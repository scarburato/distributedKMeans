% !TeX spellcheck = en_US
\documentclass[parskip=full]{report}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{listings}
%\usepackage{beramono}
\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{xcolor}
\usepackage[a4paper, margin={3cm}]{geometry}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{svg}
\usepackage{subcaption}
\usepackage{float}
\usepackage{pdfpages}
\usepackage{algorithmicx}
\usepackage[ruled]{algorithm}
\usepackage{algpseudocode}
\usepackage{pgfplots}

\usepackage{tikz}

\usepackage{hyphenat}
\usepackage[english]{babel}
% Carattere monospaziato di default
\renewcommand{\ttdefault}{pcr}

\tikzstyle{block} = [draw, fill=blue!20, rectangle, 
minimum height=3em, minimum width=6em]
\tikzstyle{sum} = [draw, fill=blue!20, circle, node distance=1cm]
\tikzstyle{input} = [coordinate]
\tikzstyle{output} = [coordinate]
\tikzstyle{pinstyle} = [pin edge={to-,thin,black}]

\lstset{
	% wrap long lines on new line
	postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
	breaklines=true, 
	columns=fullflexible,
	% tab and fonts
	tabsize=2,
	basicstyle=\ttfamily\small,
	% theme
	numbers=left,
	rulecolor=\color{black!30},	
	% UTF8 and escape
	escapeinside={\%TEX}{\^^M},
	inputencoding=utf8,
	extendedchars=true,
	literate={á}{{\'a}}1 {à}{{\`a}}1 {é}{{\'e}}1 {è}{{\`e}}1,
}


% Title Page
\title{
	\includegraphics[width=0.333\textwidth]{assets/unipi1.png} \\
	\textsc{University of Pisa} \\
	\vspace{.5cm}
	Artificial Intelligence and Data Engineering \\
	Cloud Computing \\
	\vspace{2cm}
	{\huge \textit{K-Means} on MapReduce}
}

\author{
	The \textbf{Don Matteo} group: \\
	\vspace{.3cm} \\
	\begin{tabular}{lr}
		Dario Pagani & 585281 \\
		Ricky Marinsalda & 585094 \\
		Giulio Bello & 603078
	\end{tabular}
}

\begin{document}
\maketitle
\tableofcontents


\chapter{Algorithm to select initial centroids}

\section{Idea}

\paragraph{}
To run the \textit{K-means} algorithm is necessary to select $k$ initial centroids, they can either be selected statically by the user --- that is as an algorithm's parameter --- or stochastically from the data; in the latter case they could either be drawn randomly with uniform probability or by employing a probability function that changes the likelihood to draw an element accordingly to certain metrics. For simplicity's sake our implementation chooses the $k$ initial point with equal probability.

\paragraph{}
We used a \textit{Map-Reduce} procedure to draw those those $k$ point

\section{Implementation}

\paragraph{Key}
This \textit{MapReduce} procedure doesn't use a key, so only one reducer will
be spawned by the framework, in our \textit{Hadoop} implementation we used the
\texttt{NullWritable} data-type as output key to reduce traffic.

\paragraph{Mapper}
The mapper assigns a random label to each data point, this value is not be confused with the \textit{MapReduce}'s key; finally it emits the tuple made of the label and the data point.

\paragraph{Combiner and Reducer}
Then, the combiner sorts the tuples by their label and emits the first $k$
smallest labels and their samples. Finally, the reducer performs the same
operations as the combiner, emitting $k$ values.

\paragraph{Probability}
If the data are split equally among the nodes and the random numbers generator generates all numbers with equal probability, then all dataset's samples have circa equal probability to be drawn.

\paragraph{Complexity}
Let $n = |D|$ the number of samples, $N$ the number of nodes and $k$ the number of samples to draw; then the time complexity is

\[
T \in \mathcal{O} \left(\dfrac{n}{N} \cdot \left(1 + \log(k)\right) + k \right)
\]

and the space complexity is

\[
S \in \Theta \left(k\right)
\]

if a sorted data structure is used to store only the first $k$ smallest labels
and their associated data. The I/O complexity is optimal as we perform only linear scans of the file.

\begin{algorithm}[H]
	\caption{Random select}\label{alg:random_map}
	\begin{algorithmic}
		\Require $k \in \mathbb{N}^+$
		\Procedure{Mapper}{nLine, $r$}
			\State $I \gets \Call{rand}{\;}$
				\Comment{Assign a random number to each file's row}
			\State \Return $\left< \texttt{null}, \left< I, r\right> \right>$
				\Comment{Constant key for all lines}
		\EndProcedure
		\vspace{.25cm}
		\Procedure{Combiner}{$S$}
			\State $L$ is a data structure \textbf{ordered} by key $I$
				\Comment{E.g. a binary search tree}
			\State $L \gets \emptyset$
			\While{$S \neq \emptyset$}
				\State $\left< \texttt{null}, \left< I, r\right> \right> \
					\gets \Call{read}{S}$
					\Comment{Read data from the mapper (or combiners)}
				%\State $L \gets L \bigcup \left\{ \left< I, r\right>\right\}$
				\State \Call{insert}{$L$, $\left< I, r\right>$}
				\If{$|L| > k$}
					\Comment{We store at most $k + 1$ elements}
					\State \Call{pop\_last}{L}
					\Comment{We remove the last element (sorted by $I$)}
				\EndIf

				\State \Call{next}{$S$}
			\EndWhile
			\ForAll{$ \left< I, r\right> \in L$}
				\State \Call{emitt}{$\left< \texttt{null}, \left< I, r\right> \right>$}
			\EndFor
		\EndProcedure
		\vspace{.25cm}
		\Procedure{Reducer}{$S$}
			\State \Call{Combiner}{$S$} \Comment{Same as the combiner}
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\chapter{K-means}

	The following two classes, \texttt{Point} and \texttt{AverageBuilder}, are utilized to facilitate the required computations to implement the K-means algorithm using the MapReduce paradigm.

	\section{\texttt{Point} Class}

	The \texttt{Point} class represents a point in a $d$-dimensional space. It 
	offers the following functionalities:

	\begin{itemize}
		\item Setting the components of a point, performing mathematical operations (addition, subtraction, scalar multiplication), calculating distance, and converting to/from string representation.
		\item Implementation of the \texttt{Writable} interface to support serialization and deserialization.

	\end{itemize}

	\section{\texttt{AverageBuilder} Class}

	The \texttt{AverageBuilder} class aids in computing the average of multiple points. It provides the following features:

	\begin{itemize}
		\item Storage of the sum of points and their cardinality (number of points).
		\item Methods to add points to the computation, either individually or by combining with another \texttt{AverageBuilder} object.
		\item Computation and retrieval of the average point.
		\item Implementation of the \texttt{Writable} interface for serialization and deserialization.
	\end{itemize}

	The \texttt{Point} class handles point-related operations in the Mapper and Reduce classes, while the \texttt{AverageBuilder} class facilitates efficient computation of the average of multiple points used in the combiner and reducer.



\section{Pseudo-code}

\subsection{Mapper}
The map phase of the k-means algorithm is responsible for assigning each data point to the nearest cluster center. It involves calculating the distance between the input data point and each centroid, where all centroids are stored globally and accessible to every mapper. The map phase keeps track of the index of the cluster center that has the minimum distance to the data point. The output of the map function is a key-value pair, where the key represents the index of the nearest cluster center, and the value remains as the data point itself.

\paragraph{Input}
The KMeansMapper algorithm takes three parameters as input:
\begin{itemize}
	\item \textit{key}: The key represents the identifier of the data point.
	\item \textit{p}: The data point to be assigned to a centroid.
	\item \textit{centroids}: A list of centroids representing the current centers of the clusters.
\end{itemize}


\paragraph{Output}
The algorithm produces a single output, which is a tuple \((\text{closestCentroid}, p)\). The \textit{closestCentroid} is the index of the centroid that is closest to the data point \textit{p}. The \textit{p} is the original data point.

\paragraph{Example Usage}
Suppose we have a K-means clustering problem with three clusters and the following data point and centroids:

Data point: \textit{p} = [1, 2, 3]

Centroids: [ [4, 5, 6], [7, 8, 9], [10, 11, 12] ]

Applying the KMeansMapper algorithm to the data point and centroids, we get the following result:

\texttt{Map(key, p, centroids) -> (1, [1, 2, 3])}

This means that the data point \textit{p} is closest to the centroid at index 1 ([7, 8, 9]).

\paragraph{Complexity Analysis}
The time complexity of the KMeansMapper algorithm depends on the number of centroids in the \textit{centroids} list. Let's denote this number as \textit{k}. In each iteration of the for loop, the algorithm calculates the distance between the data point \textit{p} and a centroid, resulting in a time complexity of \(O(k)\). Overall, the time complexity of the algorithm is \(O(k)\), where \(k\) is the number of centroids.

The space complexity of the algorithm is \(O(1)\) since it uses a constant amount of additional space to store the variables \textit{minDistance}, \textit{closestCentroid}, and the output tuple.


	\begin{algorithm}
		\caption{KMeansMapper}
		\begin{algorithmic}[1]


			\Procedure{Map}{key, $p$, centroids}
			\State minDistance $\gets \infty$
			\State closestCentroid $\gets -1$
			\For{$i \gets 0$ \textbf{to} length(centroids)-1}
			\State distance $\gets$ calculateDistance(centroids[$i$], $p$)
			\If{distance $\geq$ minDistance}
			\State \textbf{continue}
			\EndIf
			\State closestCentroid $\gets i$
			\State minDistance $\gets$ distance
			\EndFor
			\State \textbf{return} (closestCentroid, $p$)

			\EndProcedure
		\end{algorithmic}
	\end{algorithm}


\subsection{Reducer}

During this stage, the reduce function calculates the new centers for each cluster by averaging the data points assigned to the clusters.. It is worth mentioning that our implementation of the combiner and reducer is identical because we chose to implement the combiner in a conventional manner. This means that the combiner acts as a local copy of the reducer and its role is to process the output data of a single mapper, thereby reducing the computational burden on the reducer.

\paragraph{Input}

The \textbf{KMeansReducer} algorithm takes two parameters as input:
\begin{itemize}
	\item \textit{key}: The key represents the cluster identifier or label.
	\item \textit{values}: A list of values corresponding to the data points assigned to the cluster represented by the key.
\end{itemize}

\paragraph{Output}

The algorithm produces a single output, which is a tuple \textit{(key, new\_center)}. The \textit{key} remains the same as the input key, representing the cluster identifier. The \textit{new\_center} is the calculated new center for the cluster based on the assigned data points.

\paragraph{Example Usage}

Suppose we have a K-means clustering problem with three clusters labeled as A, B, and C. After the initial assignment of data points to clusters, we have the following data points assigned to each cluster:

\begin{itemize}
	\item Cluster A: [3, 4, 5]
	\item Cluster B: [1, 2]
	\item Cluster C: [6, 7, 8, 9]
\end{itemize}

Applying the \textbf{KMeansReducer} algorithm to each cluster, we get the following new centers:

\begin{itemize}
	\item \textbf{reduce}("A", [3, 4, 5]) $\rightarrow$ ("A", 4)
	\item \textbf{reduce}("B", [1, 2]) $\rightarrow$ ("B", 1.5)
	\item \textbf{reduce}("C", [6, 7, 8, 9]) $\rightarrow$ ("C", 7.5)
\end{itemize}

The new centers can then be used in the next iteration of the K-means algorithm to update the cluster assignments based on the proximity to these centers.

\paragraph{Complexity Analysis}

The time complexity of the \textbf{KMeansReducer} algorithm depends on the size of the $values$ list, which represents the number of data points assigned to a particular cluster. Assuming the number of data points assigned to each cluster is approximately the same, the time complexity can be considered linear, $O(n)$, where $n$ is the number of data points assigned to the cluster.

The space complexity of the algorithm is $O(1)$ since it uses a constant amount of additional space to store the variables $sum$, $cardinality$, and the output tuple.


\begin{algorithm}[H]
	\caption{KMeansReducer}\label{algo:kmeans-reducer}
	\begin{algorithmic}[1]
		\Procedure{reduce}{key, values}
		\State sum $\gets$ 0
		\State cardinality $\gets$ 0
		\For{\textbf{each} value \textbf{in} values}
		\State sum $\gets$ sum + value
		\State cardinality $\gets$ cardinality + 1
		\EndFor

		\State new\_center $\gets$  $\dfrac{sum}{cardinality}$
		\State \textbf{return} (key, new\_center)
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\chapter{Results}

\section{Synthetic data}

\paragraph{Centroids}
To test our program we had to generate synthetic data. We wrote a program that 
generates $k$ points $c_1, \dots, c_k$, the centroids, by drawing them from a 
random variable 
$C \in \left[-B,B\right]^d$ that has a continuous uniform distribution of 
probability in its domain. 

\paragraph{Cluster's data}
For each generated centroid $c_i$, we generate a \emph{Gaussian standard 
vector} $X_i \sim \mathcal{N}_d(\mu, \Sigma)$ with mean $\mu = c_i$ and the 
variance of each component 
$\sigma_{jj}$ with $j = 1, \dots, d$ is 
drawn from a random variable with uniform distribution of probability, this 
allows the program to generate ellipsoidal clusters. Finally, the program 
iterates through all centroids' random vectors $X_0, ..., X_k$ until it 
generates $n$ points, those points are streamed to \texttt{stdout} and written 
to a file.

\paragraph{Comparisons}
We had to run the program several times to generate different files with 
different sizes, for comparison's sake it is necessary for the clusters' shapes 
and centroids to be the same every time the program is ran; to achieve this it 
uses the same  \emph{seed} for its pseudo-random number generator used to 
initialize its random variables' parameters.

\section{Benchmark}

\begin{figure}[H]
	\centering
	\begin{tikzpicture}
		\begin{axis}[
			xmode=log,
			ymode=log,
			ymin=300,
			ymax=20000,
			xlabel={$n$},
			ylabel={$t [\text{s}]$}
			%log ticks with fixed point,
			% for log axes, x filter operates on LOGS.
			% and log(x * 1000) = log(x) + log(1000):
			%x filter/.code=\pgfmathparse{#1 + 6.90775527898214},
			]
			\addplot table [x=size, y=avg, col sep=comma] {../bench/k3mean.csv};
			\addplot +[mark=none] coordinates {(3500000, 300) (3500000, 20000)};
		\end{axis}
	\end{tikzpicture}
	\caption{$k=3$, $d=6$, red line when $|F| \ge 128\text{MB}$}
\end{figure}

\paragraph{Results}
We observe, as expected, a linear growth of the execution time with respect to 
$n$; but only after the file's size exceeds the chunk limit of $128\text{MB}$, 
with $k = 3$ it's when $n > 3 \times 10^6$
\footnote{
	Note that we're using 
	logarithmic scales, so a linear relation will 
	be represented as a line of slope $1$ and offset $log_{10}(m)$, where $m$ 
	is the slope of the linear relation.
}

\end{document}          
